{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e128be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "print(\"okay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62859a49",
   "metadata": {},
   "source": [
    "### This is the training file for non paired experiment (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1edbda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YunTian\\anaconda3\\envs\\ampligraph\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, time\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, LongformerForSequenceClassification\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e8c4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "import random\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec62852",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./final_train.csv\")\n",
    "train_label_df = pd.read_csv(\"./final_train_labels.csv\")\n",
    "train_df = train_df[['query', 'p1', 'p2', 'p3']]\n",
    "train_label_df = train_label_df[['label']]\n",
    "\n",
    "val_df = pd.read_csv(\"./final_val_text.csv\")\n",
    "val_label_df = pd.read_csv(\"./final_val_labels.csv\")\n",
    "val_df = val_df[['query', 'p1', 'p2', 'p3']]\n",
    "val_label_df = val_label_df[['label']]\n",
    "\n",
    "test_df = pd.read_csv(\"./final_test_text.csv\")\n",
    "test_label_df = pd.read_csv(\"./final_test_labels.csv\")\n",
    "test_df = test_df[['query', 'p1', 'p2', 'p3']]\n",
    "test_label_df = test_label_df[['label']]\n",
    "\n",
    "\n",
    "df_train = pd.concat([train_df, train_label_df], axis=1)\n",
    "df_val = pd.concat([val_df, val_label_df], axis=1)\n",
    "df_test = pd.concat([test_df, test_label_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980ed0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "347c5131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Datasets\n",
      "-------- Creating data  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YunTian\\anaconda3\\envs\\ampligraph\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected: 2075\n",
      "Dataset size: 15210\n",
      "-------- Train data created --------\n",
      "\n",
      "Rejected: 2473\n",
      "Dataset size: 1231\n",
      "-------- Val data created --------\n",
      "\n",
      "Datasets loaded\n",
      "Generating dataloader\n",
      "Generated dataloaders\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# data set generator for single query\n",
    "class TrainingDataBert(Dataset):\n",
    "    def __init__(self, train_df, val_df, tokenizer):\n",
    "        print(\"Loading Datasets\")\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "\n",
    "        self.rejected = 0\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        self.count = 0\n",
    "        self.init_data()\n",
    "        print(\"Datasets loaded\")\n",
    "        \n",
    "    def init_data(self):\n",
    "        print(\"-\"*8+\" Creating data  \"+\"-\"*8)\n",
    "        self.train_data = self.load_data(self.train_df)\n",
    "        print(\"-\"*8+\" Train data created \"+\"-\"*8+\"\\n\")\n",
    "        self.val_data = self.load_data(self.val_df)\n",
    "        print(\"-\"*8+\" Val data created \"+\"-\"*8+\"\\n\")\n",
    "        \n",
    "    def load_data(self, df):\n",
    "        def create_ids_and_mask(premise, hypothesis, label):\n",
    "            # outputs = self.tokenizer([hypothesis, premise], add_special_tokens=True, max_length = 512, pad_to_max_length=True, truncation=True,)\n",
    "            outputs = self.tokenizer([premise], add_special_tokens=True, max_length = 512, pad_to_max_length=True, truncation=True,)\n",
    "            attention_mask_ids = outputs['attention_mask'][0]\n",
    "            token_ids = outputs['input_ids'][0]\n",
    "            \n",
    "            final_token_ids.append(token_ids)\n",
    "            final_mask_ids.append(attention_mask_ids)\n",
    "            final_y.append(label)\n",
    "            \n",
    "        MAX_LEN = 512 - 3\n",
    "        final_token_ids = []\n",
    "        final_mask_ids = []\n",
    "        final_y = []\n",
    "    \n",
    "        premise_list = df['query'].to_list()\n",
    "        hypothesis1_list = df['p1'].to_list()\n",
    "        hypothesis2_list = df['p2'].to_list()\n",
    "        hypothesis3_list = df['p3'].to_list()\n",
    "        label_list = df['label'].to_list()\n",
    "        \n",
    "        for (premise, p1, p2, p3, label) in zip(premise_list, hypothesis1_list, hypothesis2_list, hypothesis3_list, label_list):\n",
    "            len1 = len(premise) + len(p1)\n",
    "            len2 = len(premise)+ len(p2)\n",
    "            len3 = len(premise)+ len(p3)\n",
    "            if (len1 <= MAX_LEN and len2 <= MAX_LEN and len3 <= MAX_LEN):\n",
    "                create_ids_and_mask(premise=p1, hypothesis=premise, label=label)\n",
    "                self.count += 1\n",
    "            else:\n",
    "                self.rejected += 1\n",
    "    \n",
    "        final_token_ids = torch.tensor(final_token_ids)\n",
    "        final_mask_ids = torch.tensor(final_mask_ids)\n",
    "        final_y = torch.tensor(final_y)\n",
    "\n",
    "        dataset = TensorDataset(final_token_ids, final_mask_ids, final_y)\n",
    "        print(\"Rejected:\",self.rejected)\n",
    "        print(\"Dataset size:\", df.shape[0]-self.rejected)\n",
    "        return dataset\n",
    "    \n",
    "    def get_data_loaders(self, bs, shuffle=True):\n",
    "        print(\"Generating dataloader\")\n",
    "        train_loader = DataLoader(\n",
    "          self.train_data,\n",
    "          shuffle=shuffle,\n",
    "          batch_size=bs,\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "          self.val_data,\n",
    "          shuffle=shuffle,\n",
    "          batch_size=bs,\n",
    "        )\n",
    "        print(\"Generated dataloaders\")\n",
    "        return train_loader, val_loader\n",
    "\n",
    "dataset = TrainingDataBert(df_train, df_val, tokenizer)\n",
    "train_dataset, val_dataset = dataset.get_data_loaders(bs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4ba10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import time\n",
    "\n",
    "num_epochs = 10\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "def multi_acc(y_pred, y_test):\n",
    "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer):\n",
    "  print(\"-\"*8+\" start training \"+\"*\"*8)\n",
    "  best_valid_loss = float('inf')\n",
    "  total_step = len(train_loader)\n",
    "    \n",
    "  for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    \n",
    "    print(\"Training phase:\",epoch)\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      torch.cuda.empty_cache()\n",
    "      \n",
    "      if batch_idx % 100 == 0 and not batch_idx == 0:\n",
    "          print('  Batch {:>5,}  of  {:>5,}.'.format(batch_idx, len(train_loader)))\n",
    "          print(\"Avg Loss:\",str(total_train_loss/batch_idx), \" - \", \"Avg Acc:\",str(total_train_acc/batch_idx))\n",
    "        \n",
    "      text_ids, mask_ids, y = [r.to(device) for r in batch]\n",
    "\n",
    "      train_loss, prediction = model(input_ids=text_ids, attention_mask=mask_ids, labels=y).values()\n",
    "\n",
    "      train_acc = multi_acc(prediction, y)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      # lr_scheduler.step()\n",
    "       \n",
    "      total_train_loss += train_loss.item()\n",
    "      total_train_acc  += train_acc.item()\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "    \n",
    "    print(\"Evaluation phase:\",epoch)\n",
    "    model.eval()\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, batch in enumerate(val_loader):\n",
    "        torch.cuda.empty_cache()\n",
    "        optimizer.zero_grad()\n",
    "        if batch_idx % 100 == 0 and not batch_idx == 0:\n",
    "          print('  Batch {:>5,}  of  {:>5,}.'.format(batch_idx, len(val_loader)))\n",
    "        \n",
    "        text_ids, mask_ids, y = [r.to(device) for r in batch]\n",
    "\n",
    "        loss, prediction = model(input_ids=text_ids,\n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=y).values()\n",
    "        \n",
    "        acc = multi_acc(prediction, y)\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc  += acc.item()\n",
    "    \n",
    "    val_acc  = total_val_acc/len(val_loader)\n",
    "    val_loss = total_val_loss/len(val_loader)\n",
    "    end = time.time()\n",
    "    \n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'bert_single_query_new.pt')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"Time taken:\",str(start-end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ac1b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- start training ********\n",
      "Training phase: 0\n",
      "  Batch   100  of  2,173.\n",
      "Avg Loss: 0.6020444223284721  -  Avg Acc: 0.6757143187522888\n",
      "  Batch   200  of  2,173.\n",
      "Avg Loss: 0.5894287674501538  -  Avg Acc: 0.6842857470363378\n",
      "  Batch   300  of  2,173.\n",
      "Avg Loss: 0.5798048480103414  -  Avg Acc: 0.6957143196463584\n",
      "  Batch   400  of  2,173.\n",
      "Avg Loss: 0.5719570819102228  -  Avg Acc: 0.7039286049455404\n",
      "  Batch   500  of  2,173.\n",
      "Avg Loss: 0.5610306170135736  -  Avg Acc: 0.7097143183350563\n",
      "  Batch   600  of  2,173.\n",
      "Avg Loss: 0.5556943279132247  -  Avg Acc: 0.7176190807422003\n",
      "  Batch   700  of  2,173.\n",
      "Avg Loss: 0.5536616097816399  -  Avg Acc: 0.720612277175699\n",
      "  Batch   800  of  2,173.\n",
      "Avg Loss: 0.5462415262684226  -  Avg Acc: 0.7258928895369172\n",
      "  Batch   900  of  2,173.\n",
      "Avg Loss: 0.5424625481168429  -  Avg Acc: 0.7290476511253251\n",
      "  Batch 1,000  of  2,173.\n",
      "Avg Loss: 0.5384855469167232  -  Avg Acc: 0.7308571749031544\n",
      "  Batch 1,100  of  2,173.\n",
      "Avg Loss: 0.5338805542547594  -  Avg Acc: 0.7340260062705387\n",
      "  Batch 1,200  of  2,173.\n",
      "Avg Loss: 0.5317493102575341  -  Avg Acc: 0.7360714608927568\n",
      "  Batch 1,300  of  2,173.\n",
      "Avg Loss: 0.528785579622938  -  Avg Acc: 0.7395604716585232\n",
      "  Batch 1,400  of  2,173.\n",
      "Avg Loss: 0.5288918127279196  -  Avg Acc: 0.7403061546811036\n",
      "  Batch 1,500  of  2,173.\n",
      "Avg Loss: 0.5263509922077259  -  Avg Acc: 0.7427619369427363\n",
      "  Batch 1,600  of  2,173.\n",
      "Avg Loss: 0.5250573948817328  -  Avg Acc: 0.7447321749292314\n",
      "  Batch 1,700  of  2,173.\n",
      "Avg Loss: 0.5202453886060153  -  Avg Acc: 0.7483193597548148\n",
      "  Batch 1,800  of  2,173.\n",
      "Avg Loss: 0.518996352776885  -  Avg Acc: 0.7494444764488273\n",
      "  Batch 1,900  of  2,173.\n",
      "Avg Loss: 0.516224199451114  -  Avg Acc: 0.7518797313853314\n",
      "  Batch 2,000  of  2,173.\n",
      "Avg Loss: 0.5133350064791738  -  Avg Acc: 0.7533571752011776\n",
      "  Batch 2,100  of  2,173.\n",
      "Avg Loss: 0.512054805354703  -  Avg Acc: 0.7540136378222988\n",
      "Evaluation phase: 0\n",
      "  Batch   100  of    473.\n",
      "  Batch   200  of    473.\n",
      "  Batch   300  of    473.\n",
      "  Batch   400  of    473.\n",
      "Epoch 1: train_loss: 0.5100 train_acc: 0.7546 | val_loss: 0.4534 val_acc: 0.7949\n",
      "Time taken: -698.3912992477417\n",
      "Training phase: 1\n",
      "  Batch   100  of  2,173.\n",
      "Avg Loss: 0.4293319969624281  -  Avg Acc: 0.8242857432365418\n",
      "  Batch   200  of  2,173.\n",
      "Avg Loss: 0.40379480147734287  -  Avg Acc: 0.831428602039814\n",
      "  Batch   300  of  2,173.\n",
      "Avg Loss: 0.3975408969695369  -  Avg Acc: 0.832380983432134\n",
      "  Batch   400  of  2,173.\n",
      "Avg Loss: 0.39487422423437235  -  Avg Acc: 0.8285714598000049\n",
      "  Batch   500  of  2,173.\n",
      "Avg Loss: 0.39999242972582577  -  Avg Acc: 0.8271428891420365\n",
      "  Batch   600  of  2,173.\n",
      "Avg Loss: 0.40093835211669404  -  Avg Acc: 0.8273809852202734\n",
      "  Batch   700  of  2,173.\n",
      "Avg Loss: 0.39987758371979  -  Avg Acc: 0.8300000334637506\n",
      "  Batch   800  of  2,173.\n",
      "Avg Loss: 0.396377232526429  -  Avg Acc: 0.8308928905799985\n",
      "  Batch   900  of  2,173.\n",
      "Avg Loss: 0.3961138236646851  -  Avg Acc: 0.8304762243562275\n",
      "  Batch 1,000  of  2,173.\n",
      "Avg Loss: 0.3969202150925994  -  Avg Acc: 0.8302857481837272\n",
      "  Batch 1,100  of  2,173.\n",
      "Avg Loss: 0.4001460273496129  -  Avg Acc: 0.8298701639067043\n",
      "  Batch 1,200  of  2,173.\n",
      "Avg Loss: 0.4008097109384835  -  Avg Acc: 0.828809558202823\n",
      "  Batch 1,300  of  2,173.\n",
      "Avg Loss: 0.40166285179555417  -  Avg Acc: 0.8275824517240891\n",
      "  Batch 1,400  of  2,173.\n",
      "Avg Loss: 0.4002033290426646  -  Avg Acc: 0.8276530955306122\n",
      "  Batch 1,500  of  2,173.\n",
      "Avg Loss: 0.400957977950573  -  Avg Acc: 0.8272381295164426\n",
      "  Batch 1,600  of  2,173.\n",
      "Avg Loss: 0.4019743747264147  -  Avg Acc: 0.826517891548574\n",
      "  Batch 1,700  of  2,173.\n",
      "Avg Loss: 0.40084467737552  -  Avg Acc: 0.8269748242988306\n",
      "  Batch 1,800  of  2,173.\n",
      "Avg Loss: 0.401880527842376  -  Avg Acc: 0.8269047963122527\n",
      "  Batch 1,900  of  2,173.\n",
      "Avg Loss: 0.40176577224935356  -  Avg Acc: 0.8268421396926829\n",
      "  Batch 2,000  of  2,173.\n",
      "Avg Loss: 0.4032039081119001  -  Avg Acc: 0.8265714631378651\n",
      "  Batch 2,100  of  2,173.\n",
      "Avg Loss: 0.4066474959183307  -  Avg Acc: 0.8240816671507699\n",
      "Evaluation phase: 1\n",
      "  Batch   100  of    473.\n",
      "  Batch   200  of    473.\n",
      "  Batch   300  of    473.\n",
      "  Batch   400  of    473.\n",
      "Epoch 2: train_loss: 0.4076 train_acc: 0.8229 | val_loss: 0.4681 val_acc: 0.8117\n",
      "Time taken: -693.2574172019958\n",
      "Training phase: 2\n",
      "  Batch   100  of  2,173.\n",
      "Avg Loss: 0.34456242993474007  -  Avg Acc: 0.8614286041259765\n",
      "  Batch   200  of  2,173.\n",
      "Avg Loss: 0.34691438285633924  -  Avg Acc: 0.865000031888485\n",
      "  Batch   300  of  2,173.\n",
      "Avg Loss: 0.33753516258051  -  Avg Acc: 0.8695238397518794\n",
      "  Batch   400  of  2,173.\n",
      "Avg Loss: 0.3316530899284407  -  Avg Acc: 0.8714286026358604\n",
      "  Batch   500  of  2,173.\n",
      "Avg Loss: 0.3211857406422496  -  Avg Acc: 0.8748571739196778\n",
      "  Batch   600  of  2,173.\n",
      "Avg Loss: 0.322216088315472  -  Avg Acc: 0.8735714601476987\n",
      "  Batch   700  of  2,173.\n",
      "Avg Loss: 0.3228820808524532  -  Avg Acc: 0.8742857464722225\n",
      "  Batch   800  of  2,173.\n",
      "Avg Loss: 0.323400104092434  -  Avg Acc: 0.8726786033809185\n",
      "  Batch   900  of  2,173.\n",
      "Avg Loss: 0.32901122737261984  -  Avg Acc: 0.8704762223694059\n",
      "  Batch 1,000  of  2,173.\n",
      "Avg Loss: 0.3308831481859088  -  Avg Acc: 0.8697143171429634\n",
      "  Batch 1,100  of  2,173.\n",
      "Avg Loss: 0.33574952807954767  -  Avg Acc: 0.86779223929752\n",
      "  Batch 1,200  of  2,173.\n",
      "Avg Loss: 0.3357226511277258  -  Avg Acc: 0.8678571742773056\n",
      "  Batch 1,300  of  2,173.\n",
      "Avg Loss: 0.33452337763487144  -  Avg Acc: 0.8679121195811492\n",
      "  Batch 1,400  of  2,173.\n",
      "Avg Loss: 0.3346566890113588  -  Avg Acc: 0.8676530931251389\n",
      "  Batch 1,500  of  2,173.\n",
      "Avg Loss: 0.3364951316776375  -  Avg Acc: 0.8661905083258947\n",
      "  Batch 1,600  of  2,173.\n",
      "Avg Loss: 0.33776460161083377  -  Avg Acc: 0.8654464606568217\n",
      "  Batch 1,700  of  2,173.\n",
      "Avg Loss: 0.33750889760058594  -  Avg Acc: 0.865294149518013\n",
      "  Batch 1,800  of  2,173.\n",
      "Avg Loss: 0.3400715517387208  -  Avg Acc: 0.8642857462830014\n",
      "  Batch 1,900  of  2,173.\n",
      "Avg Loss: 0.33936835631041934  -  Avg Acc: 0.8648120620376185\n",
      "  Batch 2,000  of  2,173.\n",
      "Avg Loss: 0.34378563923109323  -  Avg Acc: 0.8631428891122341\n",
      "  Batch 2,100  of  2,173.\n",
      "Avg Loss: 0.34549921180077253  -  Avg Acc: 0.8622449298983529\n",
      "Evaluation phase: 2\n",
      "  Batch   100  of    473.\n",
      "  Batch   200  of    473.\n",
      "  Batch   300  of    473.\n",
      "  Batch   400  of    473.\n",
      "Epoch 3: train_loss: 0.3451 train_acc: 0.8625 | val_loss: 0.5190 val_acc: 0.7998\n",
      "Time taken: -698.0078775882721\n",
      "Training phase: 3\n",
      "  Batch   100  of  2,173.\n",
      "Avg Loss: 0.3269576819241047  -  Avg Acc: 0.8828571712970734\n",
      "  Batch   200  of  2,173.\n",
      "Avg Loss: 0.2990244613308459  -  Avg Acc: 0.8907143142819405\n",
      "  Batch   300  of  2,173.\n",
      "Avg Loss: 0.3051138163047532  -  Avg Acc: 0.8838095517953237\n",
      "  Batch   400  of  2,173.\n",
      "Avg Loss: 0.2956060743890703  -  Avg Acc: 0.8878571708500386\n",
      "  Batch   500  of  2,173.\n",
      "Avg Loss: 0.30003229942917825  -  Avg Acc: 0.8860000287294387\n",
      "  Batch   600  of  2,173.\n",
      "Avg Loss: 0.3018094621474544  -  Avg Acc: 0.8869047904014588\n",
      "  Batch   700  of  2,173.\n",
      "Avg Loss: 0.2958861677109131  -  Avg Acc: 0.8883673758166177\n",
      "  Batch   800  of  2,173.\n",
      "Avg Loss: 0.29868947477662006  -  Avg Acc: 0.8873214574158191\n",
      "  Batch   900  of  2,173.\n",
      "Avg Loss: 0.3025540109361625  -  Avg Acc: 0.8839682834015952\n",
      "  Batch 1,000  of  2,173.\n",
      "Avg Loss: 0.30481780956033616  -  Avg Acc: 0.8835714585781097\n",
      "  Batch 1,100  of  2,173.\n",
      "Avg Loss: 0.3049992831330746  -  Avg Acc: 0.8825974332744425\n",
      "  Batch 1,200  of  2,173.\n",
      "Avg Loss: 0.3044240030654085  -  Avg Acc: 0.8826190784573555\n",
      "  Batch 1,300  of  2,173.\n",
      "Avg Loss: 0.3036537833433025  -  Avg Acc: 0.8829670640596977\n",
      "  Batch 1,400  of  2,173.\n",
      "Avg Loss: 0.2997717911211242  -  Avg Acc: 0.8845918674554144\n",
      "  Batch 1,500  of  2,173.\n",
      "Avg Loss: 0.29986005667534965  -  Avg Acc: 0.8858095543781916\n",
      "  Batch 1,600  of  2,173.\n",
      "Avg Loss: 0.3010464241652517  -  Avg Acc: 0.8856250308081508\n",
      "  Batch 1,700  of  2,173.\n",
      "Avg Loss: 0.3022537856379195  -  Avg Acc: 0.8846218794058351\n",
      "  Batch 1,800  of  2,173.\n",
      "Avg Loss: 0.3020943554599459  -  Avg Acc: 0.8842857449087832\n",
      "  Batch 1,900  of  2,173.\n",
      "Avg Loss: 0.30527881601188134  -  Avg Acc: 0.8822556697538024\n",
      "  Batch 2,000  of  2,173.\n",
      "Avg Loss: 0.30951995849097147  -  Avg Acc: 0.8800000306814909\n",
      "  Batch 2,100  of  2,173.\n",
      "Avg Loss: 0.3125817149952941  -  Avg Acc: 0.879523840276968\n",
      "Evaluation phase: 3\n",
      "  Batch   100  of    473.\n",
      "  Batch   200  of    473.\n",
      "  Batch   300  of    473.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   400  of    473.\n",
      "Epoch 4: train_loss: 0.3210 train_acc: 0.8752 | val_loss: 0.6381 val_acc: 0.7021\n",
      "Time taken: -693.8249332904816\n",
      "Training phase: 4\n",
      "  Batch   100  of  2,173.\n",
      "Avg Loss: 0.6763729751110077  -  Avg Acc: 0.6200000289082527\n",
      "  Batch   200  of  2,173.\n",
      "Avg Loss: 0.6708044067025185  -  Avg Acc: 0.6214285983890295\n",
      "  Batch   300  of  2,173.\n",
      "Avg Loss: 0.6683305434385935  -  Avg Acc: 0.6242857425908248\n",
      "  Batch   400  of  2,173.\n",
      "Avg Loss: 0.671298850402236  -  Avg Acc: 0.6203571715205908\n",
      "  Batch   500  of  2,173.\n",
      "Avg Loss: 0.6313201570510865  -  Avg Acc: 0.6551428859233857\n",
      "  Batch   600  of  2,173.\n",
      "Avg Loss: 0.6350631176928679  -  Avg Acc: 0.6547619339327018\n",
      "  Batch   700  of  2,173.\n",
      "Avg Loss: 0.639342508656638  -  Avg Acc: 0.6516326823404857\n",
      "  Batch   800  of  2,173.\n",
      "Avg Loss: 0.6407329254969955  -  Avg Acc: 0.6510714576207102\n",
      "  Batch   900  of  2,173.\n",
      "Avg Loss: 0.6454563433925311  -  Avg Acc: 0.6431746324400107\n",
      "  Batch 1,000  of  2,173.\n",
      "Avg Loss: 0.6490201849341393  -  Avg Acc: 0.6385714577138424\n",
      "  Batch 1,100  of  2,173.\n",
      "Avg Loss: 0.6506672793897715  -  Avg Acc: 0.6375324964929711\n",
      "  Batch 1,200  of  2,173.\n",
      "Avg Loss: 0.6527734288573265  -  Avg Acc: 0.6351190766816338\n",
      "  Batch 1,300  of  2,173.\n",
      "Avg Loss: 0.6535351769740765  -  Avg Acc: 0.633516512467311\n",
      "  Batch 1,400  of  2,173.\n",
      "Avg Loss: 0.6551737699764115  -  Avg Acc: 0.6309183962643147\n",
      "  Batch 1,500  of  2,173.\n",
      "Avg Loss: 0.65640499629577  -  Avg Acc: 0.6296190764208635\n",
      "  Batch 1,600  of  2,173.\n",
      "Avg Loss: 0.6564149608835578  -  Avg Acc: 0.6300893146172166\n",
      "  Batch 1,700  of  2,173.\n",
      "Avg Loss: 0.6570965191897224  -  Avg Acc: 0.6305042306640569\n",
      "  Batch 1,800  of  2,173.\n",
      "Avg Loss: 0.6573782869180044  -  Avg Acc: 0.630634949737125\n",
      "  Batch 1,900  of  2,173.\n",
      "Avg Loss: 0.6588060557528546  -  Avg Acc: 0.6278947659072123\n",
      "  Batch 2,000  of  2,173.\n",
      "Avg Loss: 0.6579670867174864  -  Avg Acc: 0.6297143148034811\n",
      "  Batch 2,100  of  2,173.\n",
      "Avg Loss: 0.6583282809456189  -  Avg Acc: 0.6297279200951258\n",
      "Evaluation phase: 4\n",
      "  Batch   100  of    473.\n",
      "  Batch   200  of    473.\n",
      "  Batch   300  of    473.\n",
      "  Batch   400  of    473.\n",
      "Epoch 5: train_loss: 0.6584 train_acc: 0.6301 | val_loss: 0.6623 val_acc: 0.6243\n",
      "Time taken: -699.0723307132721\n",
      "Training phase: 5\n",
      "  Batch   100  of  2,173.\n",
      "Avg Loss: 0.6657919737696648  -  Avg Acc: 0.6300000274181365\n",
      "  Batch   200  of  2,173.\n",
      "Avg Loss: 0.6671065455675125  -  Avg Acc: 0.6221428843587637\n",
      "  Batch   300  of  2,173.\n",
      "Avg Loss: 0.6670657785733541  -  Avg Acc: 0.622380980203549\n",
      "  Batch   400  of  2,173.\n",
      "Avg Loss: 0.6656274174898863  -  Avg Acc: 0.6257143132388592\n",
      "  Batch   500  of  2,173.\n",
      "Avg Loss: 0.6676425043940544  -  Avg Acc: 0.6208571715354919\n",
      "  Batch   600  of  2,173.\n",
      "Avg Loss: 0.6697717278202375  -  Avg Acc: 0.6173809811721246\n",
      "  Batch   700  of  2,173.\n",
      "Avg Loss: 0.6676684274418013  -  Avg Acc: 0.6214286001452378\n",
      "  Batch   800  of  2,173.\n",
      "Avg Loss: 0.6660679605230689  -  Avg Acc: 0.6266071717441082\n",
      "  Batch   900  of  2,173.\n",
      "Avg Loss: 0.6654448793000645  -  Avg Acc: 0.627936537116766\n",
      "  Batch 1,000  of  2,173.\n",
      "Avg Loss: 0.6648965449631214  -  Avg Acc: 0.6291428865343333\n",
      "  Batch 1,100  of  2,173.\n",
      "Avg Loss: 0.6661225283687765  -  Avg Acc: 0.6258441853523254\n",
      "  Batch 1,200  of  2,173.\n",
      "Avg Loss: 0.6666326556106409  -  Avg Acc: 0.6248809819668532\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataset, val_dataset, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrainingDataBert(df_train, df_val, tokenizer)\n",
    "train_dataset, val_dataset = dataset.get_data_loaders(bs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c6f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ampligraph]",
   "language": "python",
   "name": "conda-env-ampligraph-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

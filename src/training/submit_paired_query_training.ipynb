{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38a8eee5",
   "metadata": {},
   "source": [
    "### This is the training file and code for paired inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, time\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, LongformerForSequenceClassification\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b83252",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./final_train.csv\")\n",
    "train_label_df = pd.read_csv(\"./final_train_labels.csv\")\n",
    "train_df = train_df[['query', 'p1', 'p2', 'p3']]\n",
    "train_label_df = train_label_df[['label']]\n",
    "\n",
    "val_df = pd.read_csv(\"./final_val_text.csv\")\n",
    "val_label_df = pd.read_csv(\"./final_val_labels.csv\")\n",
    "val_df = val_df[['query', 'p1', 'p2', 'p3']]\n",
    "val_label_df = val_label_df[['label']]\n",
    "\n",
    "test_df = pd.read_csv(\"./final_test_text.csv\")\n",
    "test_label_df = pd.read_csv(\"./final_test_labels.csv\")\n",
    "test_df = test_df[['query', 'p1', 'p2', 'p3']]\n",
    "test_label_df = test_label_df[['label']]\n",
    "\n",
    "\n",
    "df_train = pd.concat([train_df, train_label_df], axis=1)\n",
    "df_val = pd.concat([val_df, val_label_df], axis=1)\n",
    "df_test = pd.concat([test_df, test_label_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class TrainingDataBert(Dataset):\n",
    "    def __init__(self, train_df, val_df, tokenizer):\n",
    "        print(\"Loading Datasets\")\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "\n",
    "        self.rejected = 0\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        self.count = 0\n",
    "        self.init_data()\n",
    "        print(\"Datasets loaded\")\n",
    "        \n",
    "    def init_data(self):\n",
    "        print(\"-\"*8+\" Creating data  \"+\"-\"*8)\n",
    "        self.train_data = self.load_data(self.train_df)\n",
    "        print(\"-\"*8+\" Train data created \"+\"-\"*8+\"\\n\")\n",
    "        self.val_data = self.load_data(self.val_df)\n",
    "        print(\"-\"*8+\" Val data created \"+\"-\"*8+\"\\n\")\n",
    "        \n",
    "    def load_data(self, df):\n",
    "        def create_ids_and_mask(premise, hypothesis, label):\n",
    "            outputs = self.tokenizer([premise, hypothesis], add_special_tokens=True, max_length = 512, pad_to_max_length=True, truncation=True,)\n",
    "            attention_mask_ids = outputs['attention_mask'][0]\n",
    "            token_ids = outputs['input_ids'][0]\n",
    "            \n",
    "            final_token_ids.append(token_ids)\n",
    "            final_mask_ids.append(attention_mask_ids)\n",
    "            final_y.append(label)\n",
    "            \n",
    "        MAX_LEN = 512 - 3\n",
    "        final_token_ids = []\n",
    "        final_mask_ids = []\n",
    "        final_y = []\n",
    "    \n",
    "        premise_list = df['query'].to_list()\n",
    "        hypothesis1_list = df['p1'].to_list()\n",
    "        hypothesis2_list = df['p2'].to_list()\n",
    "        hypothesis3_list = df['p3'].to_list()\n",
    "        label_list = df['label'].to_list()\n",
    "        \n",
    "        for (premise, p1, p2, p3, label) in zip(premise_list, hypothesis1_list, hypothesis2_list, hypothesis3_list, label_list):\n",
    "            len1 = len(premise) + len(p1)\n",
    "            len2 = len(premise)+ len(p2)\n",
    "            len3 = len(premise)+ len(p3)\n",
    "            if (len1 <= MAX_LEN and len2 <= MAX_LEN and len3 <= MAX_LEN):\n",
    "                create_ids_and_mask(premise=premise, hypothesis=p1, label=label)\n",
    "                self.count += 1\n",
    "            else:\n",
    "                self.rejected += 1\n",
    "    \n",
    "        final_token_ids = torch.tensor(final_token_ids)\n",
    "        final_mask_ids = torch.tensor(final_mask_ids)\n",
    "        final_y = torch.tensor(final_y)\n",
    "\n",
    "        dataset = TensorDataset(final_token_ids, final_mask_ids, final_y)\n",
    "        print(\"Rejected:\",self.rejected)\n",
    "        print(\"Dataset size:\", df.shape[0]-self.rejected)\n",
    "        return dataset\n",
    "    \n",
    "    def get_data_loaders(self, bs, shuffle=True):\n",
    "        print(\"Generating dataloader\")\n",
    "        train_loader = DataLoader(\n",
    "          self.train_data,\n",
    "          shuffle=shuffle,\n",
    "          batch_size=bs,\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "          self.val_data,\n",
    "          shuffle=shuffle,\n",
    "          batch_size=bs,\n",
    "        )\n",
    "        print(\"Generated dataloaders\")\n",
    "        return train_loader, val_loader\n",
    "\n",
    "dataset = TrainingDataBert(df_train, df_val, tokenizer)\n",
    "train_dataset, val_dataset = dataset.get_data_loaders(bs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ca59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import time\n",
    "\n",
    "num_epochs = 10\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "def multi_acc(y_pred, y_test):\n",
    "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer):\n",
    "  print(\"-\"*8+\" start training \"+\"*\"*8)\n",
    "  best_valid_loss = float('inf')\n",
    "  total_step = len(train_loader)\n",
    "    \n",
    "  for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "\n",
    "    print(\"Training phase:\",epoch)\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      torch.cuda.empty_cache()\n",
    "      \n",
    "      if batch_idx % 100 == 0 and not batch_idx == 0:\n",
    "          print('  Batch {:>5,}  of  {:>5,}.'.format(batch_idx, len(train_loader)))\n",
    "          print(\"Avg Loss:\",str(total_train_loss/batch_idx), \" - \", \"Avg Acc:\",str(total_train_acc/batch_idx))\n",
    "        \n",
    "      text_ids, mask_ids, y = [r.to(device) for r in batch]\n",
    "\n",
    "      train_loss, prediction = model(input_ids=text_ids, attention_mask=mask_ids, labels=y).values()\n",
    "\n",
    "      train_acc = multi_acc(prediction, y)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      # lr_scheduler.step()\n",
    "       \n",
    "      total_train_loss += train_loss.item()\n",
    "      total_train_acc  += train_acc.item()\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "    \n",
    "    print(\"Evaluation phase:\",epoch)\n",
    "    model.eval()\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, batch in enumerate(val_loader):\n",
    "        torch.cuda.empty_cache()\n",
    "        optimizer.zero_grad()\n",
    "        if batch_idx % 100 == 0 and not batch_idx == 0:\n",
    "          print('  Batch {:>5,}  of  {:>5,}.'.format(batch_idx, len(val_loader)))\n",
    "        \n",
    "        text_ids, mask_ids, y = [r.to(device) for r in batch]\n",
    "\n",
    "        loss, prediction = model(input_ids=text_ids,\n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=y).values()\n",
    "        \n",
    "        acc = multi_acc(prediction, y)\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc  += acc.item()\n",
    "    \n",
    "    val_acc  = total_val_acc/len(val_loader)\n",
    "    val_loss = total_val_loss/len(val_loader)\n",
    "    end = time.time()\n",
    "    \n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'bert_paired.pt')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"Time taken:\",str(start-end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_dataset, val_dataset, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ampligraph",
   "language": "python",
   "name": "ampligraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
